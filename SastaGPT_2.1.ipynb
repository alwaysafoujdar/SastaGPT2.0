{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYWeDeWIjFGa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "# Define model hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "CONTEXT_WINDOW = 64 # Max context length\n",
        "EPOCHS = 5000\n",
        "CHECKPOINT_INTERVAL = 500\n",
        "LR = 3e-4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "EMBEDDING_DIM = 384\n",
        "HEADS = 6\n",
        "LAYERS = 6\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# --- MoE Hyperparameters ---\n",
        "NUM_EXPERTS = 8\n",
        "TOP_K_EXPERTS = 2\n",
        "\n",
        "torch.manual_seed(457)\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    with open('stoic.txt', 'r', encoding='utf-8') as file:\n",
        "        corpus = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'stoic.txt' not found. Please create this file with your training data.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Character encoding setup\n",
        "char_list = sorted(list(set(corpus)))\n",
        "VOCAB_SIZE = len(char_list)\n",
        "char_to_index = {ch: i for i, ch in enumerate(char_list)}\n",
        "index_to_char = {i: ch for i, ch in enumerate(char_list)}\n",
        "\n",
        "encode_text = lambda s: [char_to_index[c] for c in s]\n",
        "decode_text = lambda l: ''.join([index_to_char[i] for i in l])\n",
        "\n",
        "# Train-validation split\n",
        "data_tensor = torch.tensor(encode_text(corpus), dtype=torch.long)\n",
        "split_idx = int(0.9 * len(data_tensor))\n",
        "train_data, val_data = data_tensor[:split_idx], data_tensor[split_idx:]\n",
        "\n",
        "# Function to generate mini-batches\n",
        "def get_batch(mode):\n",
        "    dataset = train_data if mode == 'train' else val_data\n",
        "    max_start_index = len(dataset) - CONTEXT_WINDOW - 1\n",
        "    if max_start_index <= 0:\n",
        "         raise ValueError(\"Dataset is too small for the given CONTEXT_WINDOW.\")\n",
        "    idxs = torch.randint(max_start_index, (BATCH_SIZE,))\n",
        "    x_batch = torch.stack([dataset[i:i + CONTEXT_WINDOW] for i in idxs])\n",
        "    y_batch = torch.stack([dataset[i + 1:i + CONTEXT_WINDOW + 1] for i in idxs])\n",
        "    return x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_loss():\n",
        "    losses = {}\n",
        "    model.eval()\n",
        "    for mode in ['train', 'val']:\n",
        "        batch_losses = torch.zeros(EVAL_ITERS)\n",
        "        for i in range(EVAL_ITERS):\n",
        "            x, y = get_batch(mode)\n",
        "            _, loss, _ = model(x, targets=y) # Pass targets directly and unpack the 3 return values\n",
        "            if loss is not None:\n",
        "                 batch_losses[i] = loss.item()\n",
        "            else:\n",
        "                 batch_losses[i] = float('nan')\n",
        "        losses[mode] = batch_losses[~torch.isnan(batch_losses)].mean()\n",
        "    model.train()\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P1Tm1LuSm3wx"
      },
      "outputs": [],
      "source": [
        "# --- Attention with KV Cache (No RoPE) ---\n",
        "\n",
        "# Define attention heads with KV Cache handling\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_dim):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.query = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.value = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        # causal mask is created dynamically in MultiHeadAttention\n",
        "\n",
        "    def forward(self, x, mask, cache_k=None, cache_v=None):\n",
        "        B, T, C = x.shape # Batch, Time (current), Channels\n",
        "        k = self.key(x)   # (B, T, head_dim)\n",
        "        q = self.query(x) # (B, T, head_dim)\n",
        "        v = self.value(x) # (B, T, head_dim)\n",
        "\n",
        "        # --- KV Cache Logic ---\n",
        "        if cache_k is not None and cache_v is not None:\n",
        "            # Concatenate past keys/values (from cache) with current key/value\n",
        "            k = torch.cat([cache_k, k], dim=1) # (B, T_prev + T_curr, head_dim)\n",
        "            v = torch.cat([cache_v, v], dim=1) # (B, T_prev + T_curr, head_dim)\n",
        "\n",
        "        # Update cache with the *new* combined k, v for the next iteration\n",
        "        updated_cache_k = k\n",
        "        updated_cache_v = v\n",
        "\n",
        "        # --- Attention Calculation ---\n",
        "        # Use the full key/value sequence (cached + current)\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) * (k.shape[-1] ** -0.5) # (B, T_curr, T_prev + T_curr)\n",
        "\n",
        "        if mask is not None:\n",
        "             # Ensure mask aligns with the attention scores dimensions (query len x key len)\n",
        "             # Mask shape should be (T_curr, T_prev + T_curr)\n",
        "             current_mask = mask[:T, :k.size(1)]\n",
        "             attention_scores = attention_scores.masked_fill(current_mask == 0, float('-inf'))\n",
        "\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1) # (B, T_curr, T_prev + T_curr)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Attend to the full value sequence\n",
        "        out = attention_probs @ v # (B, T_curr, head_dim)\n",
        "\n",
        "        return out, updated_cache_k, updated_cache_v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, EMBEDDING_DIM)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.num_heads = num_heads\n",
        "        # Register buffer for causal mask (reusable)\n",
        "        self.register_buffer('causal_mask', torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW, dtype=torch.bool)).view(1, 1, CONTEXT_WINDOW, CONTEXT_WINDOW))\n",
        "\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "        head_outputs = []\n",
        "        updated_kv_cache = [] if kv_cache is not None else None\n",
        "\n",
        "        # Determine the causal mask based on current sequence length T\n",
        "        # This mask prevents attention to future tokens within the current processing window\n",
        "        mask = self.causal_mask[:, :, :T, :T].squeeze(0).squeeze(0) # Get (T, T) mask\n",
        "\n",
        "        for i, h in enumerate(self.heads):\n",
        "            cache_k, cache_v = None, None\n",
        "            # --- Extract cache for this head ---\n",
        "            if kv_cache is not None and kv_cache[i] is not None:\n",
        "                 cache_k, cache_v = kv_cache[i]\n",
        "                 # Create the appropriate mask for when cache is present\n",
        "                 # It needs to be causal within the current query tokens (T x T)\n",
        "                 # And allow attention to all previous key tokens (T x T_prev)\n",
        "                 T_prev = cache_k.shape[1]\n",
        "                 # Mask should be (T_curr, T_prev + T_curr)\n",
        "                 full_mask = torch.ones(T, T_prev + T, dtype=torch.bool, device=x.device)\n",
        "                 causal_part = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
        "                 full_mask[:, T_prev:] = causal_part # Apply causal mask only to the current part\n",
        "                 mask = full_mask # Override the simple causal mask\n",
        "            # --- Pass relevant part of cache and mask to the head ---\n",
        "            out_h, updated_k, updated_v = h(x, mask, cache_k=cache_k, cache_v=cache_v)\n",
        "            head_outputs.append(out_h)\n",
        "            if updated_kv_cache is not None:\n",
        "                updated_kv_cache.append((updated_k, updated_v))\n",
        "\n",
        "        # Concatenate heads outputs\n",
        "        out = torch.cat(head_outputs, dim=-1) # (B, T, num_heads * head_size)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out, updated_kv_cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xo3n7l6Pm8kU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Mixture of Experts (Unchanged) ---\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, input_dim: int, num_experts: int, top_k: int):\n",
        "        super().__init__()\n",
        "        self.gate_linear = nn.Linear(input_dim, num_experts, bias=False)\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        logits = self.gate_linear(x)\n",
        "        weights = F.softmax(logits, dim=-1, dtype=torch.float)\n",
        "        top_k_weights, top_k_indices = torch.topk(weights, self.top_k, dim=-1)\n",
        "        top_k_weights_norm = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
        "        return top_k_weights_norm.type_as(x), top_k_indices\n",
        "\n",
        "class MixtureOfExperts(nn.Module):\n",
        "    def __init__(self, num_experts: int, top_k: int, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.gate = Gate(embedding_dim, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(embedding_dim) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "        x_flat = x.view(-1, C)\n",
        "        top_k_weights, top_k_indices = self.gate(x_flat)\n",
        "        final_output = torch.zeros_like(x_flat)\n",
        "        expert_outputs_buffer = torch.zeros(x_flat.size(0), self.top_k, C, device=x.device, dtype=x.dtype)\n",
        "\n",
        "        for i in range(self.num_experts):\n",
        "            token_indices, k_pos = torch.where(top_k_indices == i)\n",
        "            if token_indices.numel() > 0:\n",
        "                 expert_input = x_flat[token_indices]\n",
        "                 expert_output = self.experts[i](expert_input)\n",
        "                 expert_outputs_buffer[token_indices, k_pos] = expert_output * top_k_weights[token_indices, k_pos].unsqueeze(1)\n",
        "\n",
        "        final_output = expert_outputs_buffer.sum(dim=1)\n",
        "        return final_output.view(B, T, C)\n",
        "\n",
        "\n",
        "# --- Transformer Block (No RoPE) ---\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: Communication followed by Computation (MoE) \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, num_experts, top_k):\n",
        "        super().__init__()\n",
        "        head_size = embedding_dim // num_heads\n",
        "        self.mha = MultiHeadAttention(num_heads, head_size)\n",
        "        self.moe = MixtureOfExperts(num_experts, top_k, embedding_dim)\n",
        "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        # Multi-Head Attention part (with residual connection)\n",
        "        attn_output, updated_kv_cache = self.mha(self.ln1(x), kv_cache=kv_cache) # No freqs_cis needed\n",
        "        x = x + attn_output\n",
        "        # Mixture of Experts part (with residual connection)\n",
        "        moe_output = self.moe(self.ln2(x))\n",
        "        x = x + moe_output\n",
        "        return x, updated_kv_cache\n",
        "\n",
        "\n",
        "# Define the language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "        # --- Add standard Positional Embeddings ---\n",
        "        self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBEDDING_DIM)\n",
        "        # --- RoPE frequencies removed ---\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(EMBEDDING_DIM, HEADS, NUM_EXPERTS, TOP_K_EXPERTS) for _ in range(LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(EMBEDDING_DIM)\n",
        "        self.lm_head = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / (LAYERS**0.5)) # Scale std dev\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache_list=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        # --- Add Positional Embeddings ---\n",
        "        # Create position indices (0 to T-1)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0) # Shape (1, T)\n",
        "        pos_emb = self.position_embedding_table(pos) # Shape (1, T, C)\n",
        "        # Add token and position embeddings (pos_emb broadcasts across batch B)\n",
        "        x = tok_emb + pos_emb\n",
        "        # --- RoPE application removed ---\n",
        "\n",
        "\n",
        "        # Initialize new cache list if needed\n",
        "        new_kv_cache_list = [None] * LAYERS if kv_cache_list is not None else None\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            layer_cache = kv_cache_list[i] if kv_cache_list is not None else None\n",
        "            # Pass x and layer cache (no freqs_cis)\n",
        "            x, updated_layer_cache = block(x, kv_cache=layer_cache)\n",
        "            if new_kv_cache_list is not None:\n",
        "                 new_kv_cache_list[i] = updated_layer_cache\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, VocabSize)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B_logits, T_logits, C_logits = logits.shape\n",
        "            logits_flat = logits.view(B_logits * T_logits, C_logits)\n",
        "            targets_flat = targets.view(B_logits * T_logits)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-1)\n",
        "\n",
        "        return logits, loss, new_kv_cache_list # Return updated cache\n",
        "\n",
        "    @torch.no_grad() # Ensure no gradients are computed during generation\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generates tokens autoregressively using the KV cache.\n",
        "        idx: (B, T_initial) tensor of initial context tokens\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "\n",
        "        kv_cache = [None] * LAYERS # List of caches, one per layer\n",
        "        generated_tokens = idx\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # --- Prepare input for this step ---\n",
        "            # Use only the last token if cache is present, otherwise use context\n",
        "            # Crop context if it exceeds window size before feeding to model\n",
        "            idx_cond = generated_tokens[:, -CONTEXT_WINDOW:]\n",
        "            is_generating = kv_cache[0] is not None and kv_cache[0][0] is not None\n",
        "\n",
        "            # If we have cache, only process the *last* token of the current sequence\n",
        "            if is_generating:\n",
        "                 idx_for_forward = idx_cond[:, -1:] # Shape (B, 1)\n",
        "            else:\n",
        "                 idx_for_forward = idx_cond # Shape (B, T_initial or CONTEXT_WINDOW)\n",
        "\n",
        "            # --- Forward pass with the current token(s) and the cache ---\n",
        "            logits, _, kv_cache = self(idx_for_forward, targets=None, kv_cache_list=kv_cache)\n",
        "\n",
        "            # --- Cache Pruning (Important!) ---\n",
        "            # Prune cache if its sequence length dimension exceeds CONTEXT_WINDOW\n",
        "            if kv_cache is not None and kv_cache[0] is not None and kv_cache[0][0].shape[1] > CONTEXT_WINDOW:\n",
        "                 for i in range(LAYERS):\n",
        "                     if kv_cache[i] is not None:\n",
        "                         k_cache, v_cache = kv_cache[i]\n",
        "                         # Keep only the most recent CONTEXT_WINDOW - 1 tokens in cache\n",
        "                         # This allows space for the *next* token's K/V to be added\n",
        "                         kv_cache[i] = (k_cache[:, -(CONTEXT_WINDOW - 1):, :], v_cache[:, -(CONTEXT_WINDOW - 1):, :])\n",
        "            # ---------------------------------\n",
        "\n",
        "            # Get logits for the very last token prediction\n",
        "            logits_last = logits[:, -1, :] # (B, VocabSize)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits_last, dim=-1)\n",
        "\n",
        "            # Sample the next token index\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # Append the sampled token to the sequence\n",
        "            generated_tokens = torch.cat((generated_tokens, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        self.train() # Set model back to training mode if needed later\n",
        "        return generated_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKdXdd7Om7u8",
        "outputId": "6ea0da7e-f174-4c4e-eea7-e8af872384e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60.38 M parameters\n",
            "Starting training on cuda...\n",
            "Vocab size: 91\n",
            "Context window: 64\n",
            "Embedding dim: 384\n",
            "Layers: 6, Heads: 6\n",
            "Experts: 8, Top-K: 2\n",
            "Epoch 0: Train Loss 4.5237, Val Loss 4.5236\n",
            "--- Generating Sample ---\n",
            "\n",
            "O„;t\"yZ“;Z2j\n",
            "(!G.8K,–”69qGGByx[uT/R…Jh!aSQWtUNPe5Alcm„“Lq;d!ésH0evAL6f†O“(eVWGj[„–\n",
            ":dR[;H6DA‡IYXmnYk\n",
            "-------------------------\n",
            "Epoch 500: Train Loss 1.6634, Val Loss 1.8261\n",
            "--- Generating Sample ---\n",
            "\n",
            "pu hen, other\n",
            "dool know receiverson? As hy to bpartitelle expeasive expecters‟, ways appmatist conte\n",
            "-------------------------\n",
            "Epoch 1000: Train Loss 1.4768, Val Loss 1.6749\n",
            "--- Generating Sample ---\n",
            "\n",
            "dispoutation of domething. But in fellow by free because as the summon,\n",
            "if I am have not intelled of\n",
            "-------------------------\n",
            "Epoch 1500: Train Loss 1.3830, Val Loss 1.6018\n",
            "--- Generating Sample ---\n",
            "\n",
            "formeditance his own admiserach a worse), then, insluck at beheat they truth the worthy care. If am \n",
            "-------------------------\n",
            "Epoch 2000: Train Loss 1.3210, Val Loss 1.5519\n",
            "--- Generating Sample ---\n",
            "\n",
            "exend and attending in into it? But if it as be reaped in the fear\n",
            "his is the affair of the same of\n",
            "\n",
            "-------------------------\n",
            "Epoch 2500: Train Loss 1.2838, Val Loss 1.5288\n",
            "--- Generating Sample ---\n",
            "\n",
            "\n",
            "Of visfer postitutious stronger,‟ think which does nothing from kinto the think.\n",
            "That is the sormoo\n",
            "-------------------------\n",
            "Epoch 3000: Train Loss 1.2500, Val Loss 1.4932\n",
            "--- Generating Sample ---\n",
            "\n",
            "eleze‟s readings him.\n",
            "Make a Dividence. For what is the master of free diviners which the moment of \n",
            "-------------------------\n",
            "Epoch 3500: Train Loss 1.2317, Val Loss 1.4880\n",
            "--- Generating Sample ---\n",
            "\n",
            "any with error, and not. Keep us that the man says, is Epictetus,\n",
            "for the animal who that thou crest\n",
            "-------------------------\n",
            "Epoch 4000: Train Loss 1.2039, Val Loss 1.4841\n",
            "--- Generating Sample ---\n",
            "\n",
            "\n",
            "Let if he is discovered by thoughts of nature as the kind of the influence of this character? and t\n",
            "-------------------------\n",
            "Epoch 4500: Train Loss 1.1867, Val Loss 1.4508\n",
            "--- Generating Sample ---\n",
            "\n",
            "plain agerish? If you wish of rest show t to hight, if you are more going – and to\n",
            "go and even, and \n",
            "-------------------------\n",
            "Epoch 4999: Train Loss 1.1665, Val Loss 1.4532\n",
            "--- Generating Sample ---\n",
            "\n",
            "\n",
            "The prize of doing with put imagination expending the tribution of which sorrow\n",
            "is everything poor \n",
            "-------------------------\n",
            "Training finished.\n",
            "\n",
            "--- Final Generation Example ---\n",
            "\n",
            "them for another side, for this reason is said of this something,\n",
            "this the statue can shall look at anything after itself expect to be rack as useness are not invited with his moments, or\n",
            "weaken‟s opinions, our drinking appearance you forgotten your vice, it is\n",
            "your object, to your lamental and to go – and with any man for this reason. Lament the things which it does not fear pot for piece of naturity; and so far aside and\n",
            "arrogant things which no man sayes and to have, he truly the man who is s\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Training Setup ---\n",
        "model = LanguageModel()\n",
        "m = model.to(DEVICE)\n",
        "\n",
        "print(f\"{sum(p.numel() for p in m.parameters()) / 1e6:.2f} M parameters\")\n",
        "\n",
        "# Use AdamW optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "# Training loop\n",
        "print(f\"Starting training on {DEVICE}...\")\n",
        "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
        "print(f\"Context window: {CONTEXT_WINDOW}\")\n",
        "print(f\"Embedding dim: {EMBEDDING_DIM}\")\n",
        "print(f\"Layers: {LAYERS}, Heads: {HEADS}\")\n",
        "print(f\"Experts: {NUM_EXPERTS}, Top-K: {TOP_K_EXPERTS}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Print loss periodically\n",
        "    if epoch == 0 or epoch % CHECKPOINT_INTERVAL == 0 or epoch == EPOCHS - 1:\n",
        "        losses = compute_loss()\n",
        "        print(f\"Epoch {epoch}: Train Loss {losses.get('train', float('nan')):.4f}, Val Loss {losses.get('val', float('nan')):.4f}\")\n",
        "\n",
        "        # Generate Sample Text\n",
        "        m.eval()\n",
        "        start_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "        print(\"--- Generating Sample ---\")\n",
        "        generated_sequence = m.generate(start_context, max_new_tokens=100)\n",
        "        generated_text = decode_text(generated_sequence[0].tolist())\n",
        "        print(generated_text)\n",
        "        print(\"-------------------------\")\n",
        "        m.train()\n",
        "\n",
        "    # Get a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward pass, calculate loss\n",
        "    logits, loss, _ = model(xb, targets=yb, kv_cache_list=None) # No cache during training\n",
        "\n",
        "    if loss is None:\n",
        "        print(f\"Warning: Loss is None at epoch {epoch}. Skipping step.\")\n",
        "        continue\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Final generation example\n",
        "print(\"\\n--- Final Generation Example ---\")\n",
        "model.eval()\n",
        "start_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "generated_sequence = model.generate(start_context, max_new_tokens=500)\n",
        "print(decode_text(generated_sequence[0].tolist()))\n",
        "print(\"-----------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "UeapQM9JqxkA",
        "outputId": "9e1bc038-416a-4aa7-df60-fe6b7ce277b7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'context' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3ea7a39b982d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sasta2.0_v3_stoic.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
          ]
        }
      ],
      "source": [
        "open('sasta2.0_v3_stoic.txt', 'w').write(decode_text(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceyMjVXrvcn2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
