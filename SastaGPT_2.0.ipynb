{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXfV1Fa7qvIG",
        "outputId": "7ab29d01-c717-4524-aaf3-c447437cf7a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x781c1869aa30>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define model hyperparameters\n",
        "BATCH_SIZE = 32  # Parallel sequences processed\n",
        "CONTEXT_WINDOW = 64  # Max context length for predictions\n",
        "EPOCHS = 5000\n",
        "CHECKPOINT_INTERVAL = 500\n",
        "LR = 3e-4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "EMBEDDING_DIM = 384\n",
        "HEADS = 6\n",
        "LAYERS = 6\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "torch.manual_seed(457)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open('stoic.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "# Character encoding setup\n",
        "char_list = sorted(set(corpus))\n",
        "VOCAB_SIZE = len(char_list)\n",
        "char_to_index = {ch: i for i, ch in enumerate(char_list)}\n",
        "index_to_char = {i: ch for i, ch in enumerate(char_list)}\n",
        "\n",
        "encode_text = lambda s: [char_to_index[c] for c in s]\n",
        "decode_text = lambda l: ''.join([index_to_char[i] for i in l])\n",
        "\n",
        "# Train-validation split\n",
        "data_tensor = torch.tensor(encode_text(corpus), dtype=torch.long)\n",
        "split_idx = int(0.9 * len(data_tensor))  # We'll be training with the first 90% of the data and do validation with the rest\n",
        "train_data, val_data = data_tensor[:split_idx], data_tensor[split_idx:]"
      ],
      "metadata": {
        "id": "7peWWIUNq0LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate mini-batches\n",
        "def get_batch(mode):\n",
        "    dataset = train_data if mode == 'train' else val_data\n",
        "    idxs = torch.randint(len(dataset) - CONTEXT_WINDOW, (BATCH_SIZE,))\n",
        "    x_batch = torch.stack([dataset[i:i + CONTEXT_WINDOW] for i in idxs])\n",
        "    y_batch = torch.stack([dataset[i + 1:i + CONTEXT_WINDOW + 1] for i in idxs])\n",
        "    return x_batch.to(DEVICE), y_batch.to(DEVICE)"
      ],
      "metadata": {
        "id": "diyIniJ0q_IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def compute_loss():\n",
        "    losses = {}\n",
        "    model.eval()\n",
        "    for mode in ['train', 'val']:\n",
        "        batch_losses = torch.zeros(EVAL_ITERS)\n",
        "        for i in range(EVAL_ITERS):\n",
        "            x, y = get_batch(mode)\n",
        "            _, loss = model(x, y)\n",
        "            batch_losses[i] = loss.item()\n",
        "        losses[mode] = batch_losses.mean()\n",
        "    model.train()\n",
        "    return losses"
      ],
      "metadata": {
        "id": "PBvIbfx2rBjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.key_cache = None\n",
        "        self.value_cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.key_cache = x.clone()\n",
        "        self.value_cache = x.clone()\n",
        "        return x"
      ],
      "metadata": {
        "id": "4XxjfHABrJFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPE(nn.Module):\n",
        "    def __init__(self, context_window, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.position_embedding_table = nn.Embedding(context_window, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        return x + self.position_embedding_table(positions)"
      ],
      "metadata": {
        "id": "0aphoiborLGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define attention heads with latent attention\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_dim):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.query = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.value = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW)))\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) * k.shape[-1] ** -0.5\n",
        "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        return attention_probs @ v"
      ],
      "metadata": {
        "id": "QLmbC8j3rP4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, EMBEDDING_DIM)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "jvZdqr6GrSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.ffwd = FeedForward(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffwd(x)\n",
        "\n",
        "class MixtureOfExperts(nn.Module):\n",
        "    def __init__(self, num_experts, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.experts = nn.ModuleList([Expert(embedding_dim) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        return torch.mean(torch.stack(expert_outputs), dim=0)"
      ],
      "metadata": {
        "id": "hFcWxeQ8rW38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feed-forward block\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "pMw3MNCzrfn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main block incorporating all components\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embedding_dim, heads):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(heads, embedding_dim // heads)\n",
        "        self.ffwd = FeedForward(embedding_dim)\n",
        "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "9kTxTG-qriML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "        self.position_embedding = RoPE(CONTEXT_WINDOW, EMBEDDING_DIM)\n",
        "        self.blocks = nn.Sequential(*[Block(EMBEDDING_DIM, HEADS) for _ in range(LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(EMBEDDING_DIM)\n",
        "        self.lm_head = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
        "        self.kv_cache = KVCache()  # Add KV cache\n",
        "        self.mo_experts = MixtureOfExperts(num_experts=2, embedding_dim=EMBEDDING_DIM)  # Add mixture of experts\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding(tok_emb)\n",
        "        x = pos_emb + self.kv_cache(tok_emb)  # Apply KV cache\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        x = self.mo_experts(x)  # Apply mixture of experts\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -CONTEXT_WINDOW:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "3V8i6PM_rlCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel()\n",
        "m = model.to(DEVICE)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# Use AdamW optimizer for best results\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    if epoch % CHECKPOINT_INTERVAL == 0 or epoch == EPOCHS - 1:\n",
        "        losses = compute_loss()\n",
        "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkOYZMRhr53z",
        "outputId": "69f1e797-8a68-4f69-9fc2-e4ee5b91c0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.098331 M parameters\n",
            "step 0: train loss 4.5188, val loss 4.5200\n",
            "step 500: train loss 1.9286, val loss 2.0548\n",
            "step 1000: train loss 1.6357, val loss 1.8127\n",
            "step 1500: train loss 1.5044, val loss 1.6995\n",
            "step 2000: train loss 1.4311, val loss 1.6429\n",
            "step 2500: train loss 1.3622, val loss 1.5822\n",
            "step 3000: train loss 1.3384, val loss 1.5664\n",
            "step 3500: train loss 1.3074, val loss 1.5485\n",
            "step 4000: train loss 1.2804, val loss 1.5213\n",
            "step 4500: train loss 1.2604, val loss 1.5143\n",
            "step 4999: train loss 1.2448, val loss 1.4903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(decode_text(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "h7qwJ_LXspRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e21ce-a18d-4bf4-9c08-18191b2039cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "to his stud the world bad so. But in last nall\n",
            "and she advangratual on the whole of them to imagine to a philosophers than so\n",
            "in letter plaination, and seemed it set read into his\n",
            "infitending with with our plying impactive it formed from Ferecultus, Regood Horant they when he don possessions, we\n",
            "ought not such to any man do who lives, at fully? But because we be lot to happy thus, where is able dogs in from the tosphurden lament? why\n",
            "did I become explee with paulan, we do not cause among himself\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('sasta2.0_v2_stoic.txt', 'w').write(decode_text(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bIwMNZn668e",
        "outputId": "f15879f9-a8da-4362-a917-b811e68c135b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lo9Snapk7JkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}